from collections import Counter
from random import choices, choice, shuffle

from loguru import logger
from model import EmbeddingConfig, IsoCategory
from helpers import fit_elems_into_context
from challenge_marker_sequences import yea_sequences, nay_sequences, B, E, Q


class ChallengeEngine:

    def __init__(self, embedding_config: EmbeddingConfig):
        self.config = embedding_config

    def create_random_contexts(self, iso_categories: list[IsoCategory],
                               iso_multiplicities: list[int],
                               context_length: int) -> list[str]:
        """
        Creates sum(iso_iso_multiplicities) random contexts.
        Each one is a string of length context_length + len(Query_marker) + len(result_marker)
        [all result markers should be of the same length].
        """
        n_contexts = sum(iso_multiplicities)
        letters = self.config.alphabets
        contexts = [choices(letters, k=context_length) for _ in range(n_contexts)]
        imprinted = []

        idx = 0
        for iso_cat, iso_mult in zip(iso_categories, iso_multiplicities):
            for _ in range(iso_mult):
                ctx = contexts[idx]
                idx += 1
                form = choice(iso_cat.base_forms)
                if iso_cat.permuted_allowed:
                    shuffle(form)
                form = [self.config.HB_marker + x + self.config.HE_marker for x in form]
                imp = [[c for c in s] for s in form]  # to single letters
                fitted = fit_elems_into_context(ctx, imp)
                imprinted.append(''.join(fitted) + self.config.Query_marker + iso_cat.result_marker)

        return imprinted


def test_setup_engine():
    """
    Simple test showing the structure of the contexts (with answer suffixes).
    """
    alph = ['A', 'B', 'C', 'D']
    config = EmbeddingConfig(alphabets=alph, Query_marker='Q', HB_marker='(', HE_marker=')')

    engine = ChallengeEngine(config)

    cat1 = IsoCategory(name='cat1', base_forms=[['--', '__', '']], permuted_allowed=False, result_marker='XX')
    cat2 = IsoCategory(name='cat1', base_forms=[['**', '..', '']], permuted_allowed=True, result_marker='YY')

    xx = engine.create_random_contexts([cat1, cat2], [5, 5], context_length=20)
    for x in xx:
        print(x)


def test_full_solution_mock():
    """
    Code used for simple training and evaluation of AI sequence models.
    """
    alph = ['A', 'C', 'G', 'T']

    config = EmbeddingConfig(alphabets=alph,
                             Query_marker=Q, HB_marker=B, HE_marker=E)
    engine = ChallengeEngine(config)
    isos = [yea_sequences, nay_sequences]
    context_length = 1000

    trainset = engine.create_random_contexts(isos, [50, 50], context_length)

    # turn to tokens for fnet
    shuffle(trainset)

    solver = None  # your solver / AI sequence model

    logger.info('training begin')
    # train your solver here
    logger.info('training complete')

    # testing
    testset = engine.create_random_contexts(isos, [50, 50], context_length=context_length)
    answer_len = len(config.Query_marker) + len(isos[0].result_marker)

    cx = Counter()
    correct_predictions = 0
    for sample in testset:
        answer = sample[-answer_len:]
        sample = sample[:-answer_len]  # crop the answer, present the context only

        solution = ""  # ... `answer_len` tokens generated by the solver after seeing `sample`

        cx[solution] += 1
        logger.debug(f'{solution=}, {answer=}')
        if solution == answer:
            correct_predictions += 1

    logger.debug(f'counters: {cx}; ')
    logger.info(f'correct predictions: {correct_predictions / len(testset) * 100 : .2f}%')


if __name__ == '__main__':
    pass
